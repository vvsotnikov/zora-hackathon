{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "Копия блокнота \"VQGAN+CLIP(Updated).ipynb\"",
   "private_outputs": true,
   "provenance": [],
   "collapsed_sections": [],
   "machine_shape": "hm",
   "include_colab_link": true
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/vvsotnikov/zora-hackathon/blob/master/MetaNFT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CppIQlPhhwhs",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Generate a MetaNFT from your wallet\n",
    "\n",
    "Based on [Zora API](https://api.zora.co/) and [CLIP+VQGAN notebook](https://colab.research.google.com/github/justinjohn0306/VQGAN-CLIP/blob/main/VQGAN%2BCLIP(Updated).ipynb#scrollTo=g7EDme5RYCrt&uniqifier=1).\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "VA1PHoJrRiK9",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "#@markdown #**Licensed under the MIT License (*Double-click me to read the license agreement*)**\n",
    "#@markdown ---\n",
    "\n",
    "# Copyright (c) 2021 Katherine Crowson\n",
    "\n",
    "# Permission is hereby granted, free of charge, to any person obtaining a copy\n",
    "# of this software and associated documentation files (the \"Software\"), to deal\n",
    "# in the Software without restriction, including without limitation the rights\n",
    "# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
    "# copies of the Software, and to permit persons to whom the Software is\n",
    "# furnished to do so, subject to the following conditions:\n",
    "\n",
    "# The above copyright notice and this permission notice shall be included in\n",
    "# all copies or substantial portions of the Software.\n",
    "\n",
    "# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    "# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
    "# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
    "# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
    "# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
    "# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n",
    "# THE SOFTWARE."
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "#@markdown #**Enter your wallet address**\n",
    "\n",
    "wallet_address = \"0xcfBECd1ba28AF429C4063E33901F8c0Bd53eC9F1\"#@param {type:\"string\"}"
   ],
   "metadata": {
    "id": "KBHA52ce4q25",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "eq0-E5mjSpmP",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "#@markdown #**Check GPU type**\n",
    "#@markdown ### Factory reset runtime if you don't have the desired GPU.\n",
    "\n",
    "#@markdown ---\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#@markdown V100 = Excellent (*Available only for Colab Pro users*)\n",
    "\n",
    "#@markdown P100 = Very Good\n",
    "\n",
    "#@markdown T4 = Good (*preferred*)\n",
    "\n",
    "#@markdown K80 = Meh\n",
    "\n",
    "#@markdown P4 = (*Not Recommended*) \n",
    "\n",
    "#@markdown ---\n",
    "\n",
    "!nvidia-smi -L"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "IO09yGQNSmSd",
    "cellView": "form",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "#@markdown #**Anti-Disconnect for Google Colab**\n",
    "#@markdown ## Run this to stop it from disconnecting automatically \n",
    "#@markdown  **(disconnects anyhow after 6 - 12 hrs for using the free version of Colab.)**\n",
    "#@markdown  *(Pro users will get about 24 hrs usage time[depends])*\n",
    "#@markdown ---\n",
    "\n",
    "import IPython\n",
    "js_code = '''\n",
    "function ClickConnect(){\n",
    "console.log(\"Working\");\n",
    "document.querySelector(\"colab-toolbar-button#connect\").click()\n",
    "}\n",
    "setInterval(ClickConnect,60000)\n",
    "'''\n",
    "display(IPython.display.Javascript(js_code))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "wSfISAhyPmyp",
    "cellView": "form",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "#@markdown #**Installation of libraries**\n",
    "# @markdown This cell will take a little while because it has to download several libraries\n",
    "\n",
    "#@markdown ---\n",
    " \n",
    "print(\"Installing CLIP...\")\n",
    "!git clone https://github.com/openai/CLIP                 &> /dev/null\n",
    " \n",
    "print(\"Installing Python Libraries for AI...\")\n",
    "!git clone https://github.com/CompVis/taming-transformers &> /dev/null\n",
    "!pip install transformers                                 &> /dev/null\n",
    "!pip install ftfy regex tqdm omegaconf pytorch-lightning  &> /dev/null\n",
    "!pip install kornia                                       &> /dev/null\n",
    "!pip install einops                                       &> /dev/null\n",
    "!pip install wget                                         &> /dev/null\n",
    " \n",
    "print(\"Installing libraries for metadata management...\")\n",
    "!pip install stegano                                      &> /dev/null\n",
    "!apt install exempi                                       &> /dev/null\n",
    "!pip install python-xmp-toolkit                           &> /dev/null\n",
    "!pip install imgtag                                       &> /dev/null\n",
    "!pip install pillow==7.1.2                                &> /dev/null\n",
    " \n",
    "print(\"Installing Python libraries for creating videos...\")\n",
    "!pip install imageio-ffmpeg                               &> /dev/null\n",
    "!mkdir steps\n",
    "print(\"Installation completed.\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "FhhdWrSxQhwg",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "#@markdown #**Download model**\n",
    "\n",
    "!curl -L -o vqgan_imagenet_f16_16384.yaml -C - 'https://heibox.uni-heidelberg.de/d/a7530b09fed84f80a887/files/?p=%2Fconfigs%2Fmodel.yaml&dl=1' #ImageNet 16384\n",
    "!curl -L -o vqgan_imagenet_f16_16384.ckpt -C - 'https://heibox.uni-heidelberg.de/d/a7530b09fed84f80a887/files/?p=%2Fckpts%2Flast.ckpt&dl=1' #ImageNet 16384"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "EXMSuW2EQWsd",
    "cellView": "form",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# @title Loading libraries and definitions\n",
    " \n",
    "import argparse\n",
    "import math\n",
    "from pathlib import Path\n",
    "import sys\n",
    " \n",
    "sys.path.append('./taming-transformers')\n",
    "from IPython import display\n",
    "from base64 import b64encode\n",
    "from omegaconf import OmegaConf\n",
    "from PIL import Image\n",
    "from taming.models import cond_transformer, vqgan\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import functional as TF\n",
    "from tqdm.notebook import tqdm\n",
    " \n",
    "from CLIP import clip\n",
    "import kornia.augmentation as K\n",
    "import numpy as np\n",
    "import imageio\n",
    "from PIL import ImageFile, Image\n",
    "from imgtag import ImgTag    # metadatos \n",
    "from libxmp import *         # metadatos\n",
    "import libxmp                # metadatos\n",
    "from stegano import lsb\n",
    "import json\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    " \n",
    "def sinc(x):\n",
    "    return torch.where(x != 0, torch.sin(math.pi * x) / (math.pi * x), x.new_ones([]))\n",
    " \n",
    " \n",
    "def lanczos(x, a):\n",
    "    cond = torch.logical_and(-a < x, x < a)\n",
    "    out = torch.where(cond, sinc(x) * sinc(x/a), x.new_zeros([]))\n",
    "    return out / out.sum()\n",
    " \n",
    " \n",
    "def ramp(ratio, width):\n",
    "    n = math.ceil(width / ratio + 1)\n",
    "    out = torch.empty([n])\n",
    "    cur = 0\n",
    "    for i in range(out.shape[0]):\n",
    "        out[i] = cur\n",
    "        cur += ratio\n",
    "    return torch.cat([-out[1:].flip([0]), out])[1:-1]\n",
    " \n",
    " \n",
    "def resample(input, size, align_corners=True):\n",
    "    n, c, h, w = input.shape\n",
    "    dh, dw = size\n",
    " \n",
    "    input = input.view([n * c, 1, h, w])\n",
    " \n",
    "    if dh < h:\n",
    "        kernel_h = lanczos(ramp(dh / h, 2), 2).to(input.device, input.dtype)\n",
    "        pad_h = (kernel_h.shape[0] - 1) // 2\n",
    "        input = F.pad(input, (0, 0, pad_h, pad_h), 'reflect')\n",
    "        input = F.conv2d(input, kernel_h[None, None, :, None])\n",
    " \n",
    "    if dw < w:\n",
    "        kernel_w = lanczos(ramp(dw / w, 2), 2).to(input.device, input.dtype)\n",
    "        pad_w = (kernel_w.shape[0] - 1) // 2\n",
    "        input = F.pad(input, (pad_w, pad_w, 0, 0), 'reflect')\n",
    "        input = F.conv2d(input, kernel_w[None, None, None, :])\n",
    " \n",
    "    input = input.view([n, c, h, w])\n",
    "    return F.interpolate(input, size, mode='bicubic', align_corners=align_corners)\n",
    " \n",
    " \n",
    "class ReplaceGrad(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x_forward, x_backward):\n",
    "        ctx.shape = x_backward.shape\n",
    "        return x_forward\n",
    " \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_in):\n",
    "        return None, grad_in.sum_to_size(ctx.shape)\n",
    " \n",
    " \n",
    "replace_grad = ReplaceGrad.apply\n",
    " \n",
    " \n",
    "class ClampWithGrad(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input, min, max):\n",
    "        ctx.min = min\n",
    "        ctx.max = max\n",
    "        ctx.save_for_backward(input)\n",
    "        return input.clamp(min, max)\n",
    " \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_in):\n",
    "        input, = ctx.saved_tensors\n",
    "        return grad_in * (grad_in * (input - input.clamp(ctx.min, ctx.max)) >= 0), None, None\n",
    " \n",
    " \n",
    "clamp_with_grad = ClampWithGrad.apply\n",
    " \n",
    " \n",
    "def vector_quantize(x, codebook):\n",
    "    d = x.pow(2).sum(dim=-1, keepdim=True) + codebook.pow(2).sum(dim=1) - 2 * x @ codebook.T\n",
    "    indices = d.argmin(-1)\n",
    "    x_q = F.one_hot(indices, codebook.shape[0]).to(d.dtype) @ codebook\n",
    "    return replace_grad(x_q, x)\n",
    " \n",
    " \n",
    "class Prompt(nn.Module):\n",
    "    def __init__(self, embed, weight=1., stop=float('-inf')):\n",
    "        super().__init__()\n",
    "        self.register_buffer('embed', embed)\n",
    "        self.register_buffer('weight', torch.as_tensor(weight))\n",
    "        self.register_buffer('stop', torch.as_tensor(stop))\n",
    " \n",
    "    def forward(self, input):\n",
    "        input_normed = F.normalize(input.unsqueeze(1), dim=2)\n",
    "        embed_normed = F.normalize(self.embed.unsqueeze(0), dim=2)\n",
    "        dists = input_normed.sub(embed_normed).norm(dim=2).div(2).arcsin().pow(2).mul(2)\n",
    "        dists = dists * self.weight.sign()\n",
    "        return self.weight.abs() * replace_grad(dists, torch.maximum(dists, self.stop)).mean()\n",
    " \n",
    " \n",
    "def parse_prompt(prompt):\n",
    "    vals = prompt.rsplit(':', 2)\n",
    "    vals = vals + ['', '1', '-inf'][len(vals):]\n",
    "    return vals[0], float(vals[1]), float(vals[2])\n",
    " \n",
    " \n",
    "class MakeCutouts(nn.Module):\n",
    "    def __init__(self, cut_size, cutn, cut_pow=1.):\n",
    "        super().__init__()\n",
    "        self.cut_size = cut_size\n",
    "        self.cutn = cutn\n",
    "        self.cut_pow = cut_pow\n",
    "        self.augs = nn.Sequential(\n",
    "            K.RandomHorizontalFlip(p=0.5),\n",
    "            # K.RandomSolarize(0.01, 0.01, p=0.7),\n",
    "            K.RandomSharpness(0.3,p=0.4),\n",
    "            K.RandomAffine(degrees=30, translate=0.1, p=0.8, padding_mode='border'),\n",
    "            K.RandomPerspective(0.2,p=0.4),\n",
    "            K.ColorJitter(hue=0.01, saturation=0.01, p=0.7))\n",
    "        self.noise_fac = 0.1\n",
    " \n",
    " \n",
    "    def forward(self, input):\n",
    "        sideY, sideX = input.shape[2:4]\n",
    "        max_size = min(sideX, sideY)\n",
    "        min_size = min(sideX, sideY, self.cut_size)\n",
    "        cutouts = []\n",
    "        for _ in range(self.cutn):\n",
    "            size = int(torch.rand([])**self.cut_pow * (max_size - min_size) + min_size)\n",
    "            offsetx = torch.randint(0, sideX - size + 1, ())\n",
    "            offsety = torch.randint(0, sideY - size + 1, ())\n",
    "            cutout = input[:, :, offsety:offsety + size, offsetx:offsetx + size]\n",
    "            cutouts.append(resample(cutout, (self.cut_size, self.cut_size)))\n",
    "        batch = self.augs(torch.cat(cutouts, dim=0))\n",
    "        if self.noise_fac:\n",
    "            facs = batch.new_empty([self.cutn, 1, 1, 1]).uniform_(0, self.noise_fac)\n",
    "            batch = batch + facs * torch.randn_like(batch)\n",
    "        return batch\n",
    " \n",
    " \n",
    "def load_vqgan_model(config_path, checkpoint_path):\n",
    "    config = OmegaConf.load(config_path)\n",
    "    if config.model.target == 'taming.models.vqgan.VQModel':\n",
    "        model = vqgan.VQModel(**config.model.params)\n",
    "        model.eval().requires_grad_(False)\n",
    "        model.init_from_ckpt(checkpoint_path)\n",
    "    elif config.model.target == 'taming.models.cond_transformer.Net2NetTransformer':\n",
    "        parent_model = cond_transformer.Net2NetTransformer(**config.model.params)\n",
    "        parent_model.eval().requires_grad_(False)\n",
    "        parent_model.init_from_ckpt(checkpoint_path)\n",
    "        model = parent_model.first_stage_model\n",
    "    elif config.model.target == 'taming.models.vqgan.GumbelVQ':\n",
    "        model = vqgan.GumbelVQ(**config.model.params)\n",
    "        print(config.model.params)\n",
    "        model.eval().requires_grad_(False)\n",
    "        model.init_from_ckpt(checkpoint_path)\n",
    "    else:\n",
    "        raise ValueError(f'unknown model type: {config.model.target}')\n",
    "    del model.loss\n",
    "    return model\n",
    " \n",
    " \n",
    "def resize_image(image, out_size):\n",
    "    ratio = image.size[0] / image.size[1]\n",
    "    area = min(image.size[0] * image.size[1], out_size[0] * out_size[1])\n",
    "    size = round((area * ratio)**0.5), round((area / ratio)**0.5)\n",
    "    return image.resize(size, Image.LANCZOS)\n",
    "\n",
    "def download_img(img_url):\n",
    "    try:\n",
    "        return wget.download(img_url,out=\"input.jpg\")\n",
    "    except:\n",
    "        return\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "#@markdown ##**Scanning your wallet for NFTs with Zora API**\n",
    "import requests, json, os, hashlib, shutil\n",
    "import urllib.request\n",
    "shutil.rmtree('nft_images', ignore_errors=True)\n",
    "os.makedirs('nft_images', exist_ok=True)\n",
    "opener = urllib.request.build_opener()\n",
    "opener.addheaders = [('User-agent',\n",
    "                        'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/102.0.5005.61 Safari/537.36')]\n",
    "urllib.request.install_opener(opener)\n",
    "query = {'query': f'''query ListNFTs {{\n",
    "  tokens(\n",
    "    where: {{ownerAddresses: \"{wallet_address}\"}}\n",
    "    pagination: {{after: \"\", limit: 500}}\n",
    "  ) {{\n",
    "    pageInfo {{\n",
    "      endCursor\n",
    "      hasNextPage\n",
    "      limit\n",
    "    }}\n",
    "    nodes {{\n",
    "      token {{\n",
    "        image {{\n",
    "          url\n",
    "          mimeType\n",
    "        }}\n",
    "        collectionName\n",
    "      }}\n",
    "    }}\n",
    "  }}\n",
    "}}''',\n",
    "'operationName': 'ListNFTs'}\n",
    "r = requests.post('https://api.zora.co/graphql',\n",
    "              data=json.dumps(query),\n",
    "              headers={'content-type': 'application/json'}).json()\n",
    "i = 0\n",
    "for node in r['data']['tokens']['nodes']:\n",
    "    if node['token']['image']:\n",
    "        link = node['token']['image'].get('url')\n",
    "        if link is None:\n",
    "            continue\n",
    "        if node['token']['image'].get('mimeType') not in {'image/png', 'image/jpg', 'image/jpeg'}:\n",
    "            continue\n",
    "        if link.startswith('ipfs://'):\n",
    "            link = link.replace('ipfs://', 'https://ipfs.io/ipfs/')\n",
    "        link = link.replace('/ipfs/ipfs/', '/ipfs/')\n",
    "        path = f'nft_images/{hashlib.sha224(link.encode()).hexdigest()}'\n",
    "        try:\n",
    "            urllib.request.urlretrieve(\n",
    "                link,\n",
    "                path)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(link)\n",
    "        i += 1\n",
    "        if i == 5:\n",
    "            break\n",
    "if i == 0:\n",
    "    raise FileNotFoundError('Haven\\'t found any NFT images on this wallet :(')"
   ],
   "metadata": {
    "id": "Nw9Y8EXr5DZy",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1tthw0YaispD",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Tools for execution:\n",
    "Mainly what you will have to modify will be `texts:`, there you can place the text (s) you want to generate (separated with `|`). It is a list because you can put more than one text, and so the AI ​​tries to 'mix' the images, giving the same priority to both texts.\n",
    "\n",
    "To use an initial image to the model, you just have to upload a file to the Colab environment (in the section on the left), and then modify `init_image:` putting the exact name of the file. Example: `sample.png`\n",
    "\n",
    "You can also modify the model by changing the lines that say `model:`. Currently 1024, 16384, WikiArt, S-FLCKR and COCO-Stuff are available. To activate them you have to have downloaded them first, and then you can simply select it.\n",
    "\n",
    "You can also use `target_images`, which is basically putting one or more images on it that the AI ​​will take as a \"target\", fulfilling the same function as putting text on it. To put more than one you have to use `|` as a separator."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ZdlpRFL8UAlW",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "#@markdown #**Parameters**\n",
    "#@markdown ---\n",
    "texts = \"trending on artstation\"\n",
    "width =  300#@param {type:\"number\"}\n",
    "height =  300#@param {type:\"number\"}\n",
    "model = \"vqgan_imagenet_f16_16384\"\n",
    "images_interval =  50\n",
    "init_image = \"\"\n",
    "target_images = \"|\".join('nft_images/' + x for x in os.listdir('nft_images'))\n",
    "seed = -1\n",
    "max_iterations = 1000\n",
    "input_images = \"\"\n",
    "\n",
    "model_names={\"vqgan_imagenet_f16_16384\": 'ImageNet 16384',\"vqgan_imagenet_f16_1024\":\"ImageNet 1024\", \n",
    "                 \"wikiart_1024\":\"WikiArt 1024\", \"wikiart_16384\":\"WikiArt 16384\", \"coco\":\"COCO-Stuff\", \"faceshq\":\"FacesHQ\", \"sflckr\":\"S-FLCKR\", \"ade20k\":\"ADE20K\", \"ffhq\":\"FFHQ\", \"celebahq\":\"CelebA-HQ\", \"gumbel_8192\": \"Gumbel 8192\"}\n",
    "name_model = model_names[model]     \n",
    "\n",
    "if model == \"gumbel_8192\":\n",
    "    is_gumbel = True\n",
    "else:\n",
    "    is_gumbel = False\n",
    "\n",
    "if seed == -1:\n",
    "    seed = None\n",
    "if init_image == \"None\":\n",
    "    init_image = None\n",
    "elif init_image and init_image.lower().startswith(\"http\"):\n",
    "    init_image = download_img(init_image)\n",
    "\n",
    "\n",
    "if target_images == \"None\" or not target_images:\n",
    "    target_images = []\n",
    "else:\n",
    "    target_images = target_images.split(\"|\")\n",
    "    target_images = [image.strip() for image in target_images]\n",
    "\n",
    "if init_image or target_images != []:\n",
    "    input_images = True\n",
    "\n",
    "texts = [frase.strip() for frase in texts.split(\"|\")]\n",
    "if texts == ['']:\n",
    "    texts = []\n",
    "\n",
    "\n",
    "args = argparse.Namespace(\n",
    "    prompts=texts,\n",
    "    image_prompts=target_images,\n",
    "    noise_prompt_seeds=[],\n",
    "    noise_prompt_weights=[],\n",
    "    size=[width, height],\n",
    "    init_image=init_image,\n",
    "    init_weight=0.,\n",
    "    clip_model='ViT-B/32',\n",
    "    vqgan_config=f'{model}.yaml',\n",
    "    vqgan_checkpoint=f'{model}.ckpt',\n",
    "    step_size=0.1,\n",
    "    cutn=64,\n",
    "    cut_pow=1.,\n",
    "    display_freq=images_interval,\n",
    "    seed=seed,\n",
    ")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "g7EDme5RYCrt",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "#@markdown #**Fire up the AI**\n",
    "\n",
    "#@markdown ---\n",
    "\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "if texts:\n",
    "    print('Using texts:', texts)\n",
    "if target_images:\n",
    "    print('Using image prompts:', target_images)\n",
    "if args.seed is None:\n",
    "    seed = torch.seed()\n",
    "else:\n",
    "    seed = args.seed\n",
    "torch.manual_seed(seed)\n",
    "print('Using seed:', seed)\n",
    "\n",
    "model = load_vqgan_model(args.vqgan_config, args.vqgan_checkpoint).to(device)\n",
    "perceptor = clip.load(args.clip_model, jit=False)[0].eval().requires_grad_(False).to(device)\n",
    "\n",
    "cut_size = perceptor.visual.input_resolution\n",
    "if is_gumbel:\n",
    "    e_dim = model.quantize.embedding_dim\n",
    "else:\n",
    "    e_dim = model.quantize.e_dim\n",
    "\n",
    "f = 2**(model.decoder.num_resolutions - 1)\n",
    "make_cutouts = MakeCutouts(cut_size, args.cutn, cut_pow=args.cut_pow)\n",
    "if is_gumbel:\n",
    "    n_toks = model.quantize.n_embed\n",
    "else:\n",
    "    n_toks = model.quantize.n_e\n",
    "\n",
    "toksX, toksY = args.size[0] // f, args.size[1] // f\n",
    "sideX, sideY = toksX * f, toksY * f\n",
    "if is_gumbel:\n",
    "    z_min = model.quantize.embed.weight.min(dim=0).values[None, :, None, None]\n",
    "    z_max = model.quantize.embed.weight.max(dim=0).values[None, :, None, None]\n",
    "else:\n",
    "    z_min = model.quantize.embedding.weight.min(dim=0).values[None, :, None, None]\n",
    "    z_max = model.quantize.embedding.weight.max(dim=0).values[None, :, None, None]\n",
    "\n",
    "if args.init_image:\n",
    "    pil_image = Image.open(args.init_image).convert('RGB')\n",
    "    pil_image = pil_image.resize((sideX, sideY), Image.LANCZOS)\n",
    "    z, *_ = model.encode(TF.to_tensor(pil_image).to(device).unsqueeze(0) * 2 - 1)\n",
    "else:\n",
    "    one_hot = F.one_hot(torch.randint(n_toks, [toksY * toksX], device=device), n_toks).float()\n",
    "    if is_gumbel:\n",
    "        z = one_hot @ model.quantize.embed.weight\n",
    "    else:\n",
    "        z = one_hot @ model.quantize.embedding.weight\n",
    "    z = z.view([-1, toksY, toksX, e_dim]).permute(0, 3, 1, 2)\n",
    "z_orig = z.clone()\n",
    "z.requires_grad_(True)\n",
    "opt = optim.Adam([z], lr=args.step_size)\n",
    "\n",
    "normalize = transforms.Normalize(mean=[0.48145466, 0.4578275, 0.40821073],\n",
    "                                 std=[0.26862954, 0.26130258, 0.27577711])\n",
    "\n",
    "pMs = []\n",
    "\n",
    "for prompt in args.prompts:\n",
    "    txt, weight, stop = parse_prompt(prompt)\n",
    "    embed = perceptor.encode_text(clip.tokenize(txt).to(device)).float()\n",
    "    pMs.append(Prompt(embed, weight, stop).to(device))\n",
    "\n",
    "for prompt in args.image_prompts:\n",
    "    path, weight, stop = parse_prompt(prompt)\n",
    "    img = resize_image(Image.open(path).convert('RGB'), (sideX, sideY))\n",
    "    batch = make_cutouts(TF.to_tensor(img).unsqueeze(0).to(device))\n",
    "    embed = perceptor.encode_image(normalize(batch)).float()\n",
    "    pMs.append(Prompt(embed, weight, stop).to(device))\n",
    "\n",
    "for seed, weight in zip(args.noise_prompt_seeds, args.noise_prompt_weights):\n",
    "    gen = torch.Generator().manual_seed(seed)\n",
    "    embed = torch.empty([1, perceptor.visual.output_dim]).normal_(generator=gen)\n",
    "    pMs.append(Prompt(embed, weight).to(device))\n",
    "\n",
    "def synth(z):\n",
    "    if is_gumbel:\n",
    "        z_q = vector_quantize(z.movedim(1, 3), model.quantize.embed.weight).movedim(3, 1)\n",
    "    else:\n",
    "        z_q = vector_quantize(z.movedim(1, 3), model.quantize.embedding.weight).movedim(3, 1)\n",
    "    \n",
    "    return clamp_with_grad(model.decode(z_q).add(1).div(2), 0, 1)\n",
    "\n",
    "def add_xmp_data(nombrefichero):\n",
    "    imagen = ImgTag(filename=nombrefichero)\n",
    "    imagen.xmp.append_array_item(libxmp.consts.XMP_NS_DC, 'creator', 'VQGAN+CLIP', {\"prop_array_is_ordered\":True, \"prop_value_is_array\":True})\n",
    "    if args.prompts:\n",
    "        imagen.xmp.append_array_item(libxmp.consts.XMP_NS_DC, 'title', \" | \".join(args.prompts), {\"prop_array_is_ordered\":True, \"prop_value_is_array\":True})\n",
    "    else:\n",
    "        imagen.xmp.append_array_item(libxmp.consts.XMP_NS_DC, 'title', 'None', {\"prop_array_is_ordered\":True, \"prop_value_is_array\":True})\n",
    "    imagen.xmp.append_array_item(libxmp.consts.XMP_NS_DC, 'i', str(i), {\"prop_array_is_ordered\":True, \"prop_value_is_array\":True})\n",
    "    imagen.xmp.append_array_item(libxmp.consts.XMP_NS_DC, 'model', name_model, {\"prop_array_is_ordered\":True, \"prop_value_is_array\":True})\n",
    "    imagen.xmp.append_array_item(libxmp.consts.XMP_NS_DC, 'seed',str(seed) , {\"prop_array_is_ordered\":True, \"prop_value_is_array\":True})\n",
    "    imagen.xmp.append_array_item(libxmp.consts.XMP_NS_DC, 'input_images',str(input_images) , {\"prop_array_is_ordered\":True, \"prop_value_is_array\":True})\n",
    "    #for frases in args.prompts:\n",
    "    #    imagen.xmp.append_array_item(libxmp.consts.XMP_NS_DC, 'Prompt' ,frases, {\"prop_array_is_ordered\":True, \"prop_value_is_array\":True})\n",
    "    imagen.close()\n",
    "\n",
    "def add_stegano_data(filename):\n",
    "    data = {\n",
    "        \"title\": \" | \".join(args.prompts) if args.prompts else None,\n",
    "        \"notebook\": \"VQGAN+CLIP\",\n",
    "        \"i\": i,\n",
    "        \"model\": name_model,\n",
    "        \"seed\": str(seed),\n",
    "        \"input_images\": input_images\n",
    "    }\n",
    "    lsb.hide(filename, json.dumps(data)).save(filename)\n",
    "\n",
    "@torch.no_grad()\n",
    "def checkin(i, losses):\n",
    "    losses_str = ', '.join(f'{loss.item():g}' for loss in losses)\n",
    "    tqdm.write(f'i: {i}, loss: {sum(losses).item():g}, losses: {losses_str}')\n",
    "    out = synth(z)\n",
    "    TF.to_pil_image(out[0].cpu()).save('progress.png')\n",
    "    add_stegano_data('progress.png')\n",
    "    add_xmp_data('progress.png')\n",
    "    display.display(display.Image('progress.png'))\n",
    "\n",
    "def ascend_txt():\n",
    "    global i\n",
    "    out = synth(z)\n",
    "    iii = perceptor.encode_image(normalize(make_cutouts(out))).float()\n",
    "\n",
    "    result = []\n",
    "\n",
    "    if args.init_weight:\n",
    "        result.append(F.mse_loss(z, z_orig) * args.init_weight / 2)\n",
    "\n",
    "    for prompt in pMs:\n",
    "        result.append(prompt(iii))\n",
    "    img = np.array(out.mul(255).clamp(0, 255)[0].cpu().detach().numpy().astype(np.uint8))[:,:,:]\n",
    "    img = np.transpose(img, (1, 2, 0))\n",
    "    filename = f\"steps/{i:04}.png\"\n",
    "    imageio.imwrite(filename, np.array(img))\n",
    "    add_stegano_data(filename)\n",
    "    add_xmp_data(filename)\n",
    "    return result\n",
    "\n",
    "def train(i):\n",
    "    opt.zero_grad()\n",
    "    lossAll = ascend_txt()\n",
    "    if i % args.display_freq == 0:\n",
    "        checkin(i, lossAll)\n",
    "    loss = sum(lossAll)\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    with torch.no_grad():\n",
    "        z.copy_(z.maximum(z_min).minimum(z_max))\n",
    "\n",
    "i = 0\n",
    "try:\n",
    "    with tqdm() as pbar:\n",
    "        while True:\n",
    "            train(i)\n",
    "            if i == max_iterations:\n",
    "                break\n",
    "            i += 1\n",
    "            pbar.update()\n",
    "except KeyboardInterrupt:\n",
    "    pass"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "2V3MEyxQNQH5",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}